# Pipeline de Dados Aduaneiros (ELT na Nuvem)

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white)

## üìñ Sobre o Projeto

Este projeto implementa um pipeline de dados **ELT (Extract, Load, Transform)** completo, demonstrando pr√°ticas modernas de engenharia de dados. Inspirado pela relev√¢ncia econ√¥mica do Porto de Santos, o pipeline processa um conjunto de dados aduaneiros fict√≠cios, movendo-os de um ambiente local para uma arquitetura de dados na nuvem, pronta para an√°lise.

O objetivo principal √© construir um **Data Warehouse** robusto e modelado, capaz de suportar an√°lises de Business Intelligence e alimentar futuros modelos de Machine Learning.

## üèõÔ∏è Arquitetura do Pipeline

O fluxo de dados segue o paradigma ELT, aproveitando a escalabilidade e o poder de processamento da nuvem.

**1. Extra√ß√£o e Carregamento (EL)**

+----------------+       +-------------------+       +-------------------+
|   Arquivo CSV  | ----> | Script Python (EL)| ----> | Amazon S3         |
|     (Local)    |       | (boto3, Snowflake)|       |   (Datalake /     |
+----------------+       +-------------------+       |   Bronze Layer)   |
                                 |                     +---------+---------+
                                 |                               |
                                 |  (COPY INTO - Load)           | (COPY INTO - Load)
                                 V                               V
                         +-------------------+             +-------------------+
                         | Snowflake         | <---------  | Snowflake         |
                         | (Staging Table)   |             | (Data Warehouse / |
                         +-------------------+             |   Star Schema)    |
                                 |                         +-------------------+
                                 |
                                 | (SQL Transforms)
                                 V
                         +-------------------+
                         | Snowflake         |
                         | (Camada Anal√≠tica |
                         |   Star Schema)    |
                         +-------------------+

-   Um script Python l√™ o arquivo CSV local e o carrega (Load) para a camada Bronze do Datalake no S3.
-   Em seguida, o script orquestra o carregamento (Load) do S3 para uma tabela de Staging no Snowflake.

**2. Transforma√ß√£o (T)**
[Snowflake (Staging Table)] -> [SQL Scripts] -> [Snowflake (Camada Anal√≠tica / Star Schema)]

-   A transforma√ß√£o ocorre inteiramente dentro do Snowflake, onde scripts SQL convertem os dados brutos da tabela de Staging para um modelo anal√≠tico limpo e perform√°tico (Star Schema).

## üìÑ Modelo de Dados (Star Schema)

Para otimizar as consultas anal√≠ticas, os dados s√£o organizados em um modelo dimensional (Star Schema), separando as m√©tricas dos eventos (Fatos) do seu contexto descritivo (Dimens√µes).

* **Tabela Fato (`fato_processos_importacao`):** Armazena os dados num√©ricos de cada processo de importa√ß√£o (valores, impostos, taxas).
* **Tabelas de Dimens√£o (`dim_produtos`, `dim_paises`, `dim_portos`, etc.):** Armazenam os dados descritivos (nomes, descri√ß√µes, categorias), garantindo consist√™ncia e performance.

## üõ†Ô∏è Tecnologias e Conceitos Utilizados

-   **Linguagem:** Python 3
-   **Cloud & Banco de Dados:**
    -   **Amazon S3:** Utilizado como Datalake para armazenar os dados brutos (Camada Bronze).
    -   **Snowflake:** Utilizado como Data Warehouse na nuvem para staging, transforma√ß√£o e armazenamento da camada anal√≠tica.
    -   **SQL:** Linguagem principal para toda a etapa de Transforma√ß√£o de dados.
-   **Bibliotecas Python:**
    -   `boto3`: Para interagir com a API da AWS e fazer o upload para o S3.
    -   `snowflake-connector-python`: Para conectar e executar comandos no Snowflake.
    -   `pandas`: Para a leitura e valida√ß√£o inicial dos dados.
    -   `python-dotenv`: Para o gerenciamento seguro de credenciais.
-   **Conceitos de Engenharia de Dados:**
    -   **ELT (Extract, Load, Transform):** Arquitetura moderna de pipeline de dados.
    -   **Datalake & Data Warehouse:** Implementa√ß√£o dos dois conceitos centrais de armazenamento de dados.
    -   **Modelagem Dimensional (Star Schema):** Desenho do modelo de dados para otimiza√ß√£o anal√≠tica.
    -   **Infraestrutura como C√≥digo (IaC):** Utiliza√ß√£o de scripts para provisionar recursos (Bucket S3).

## üöÄ Como Executar o Projeto

### 1. Pr√©-requisitos
-   Python 3.8 ou superior
-   Conta na [AWS](https://aws.amazon.com/free/) com um usu√°rio IAM configurado.
-   Conta no [Snowflake](https://signup.snowflake.com/) (o trial de 30 dias √© suficiente).
-   [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) instalado e configurado (`aws configure`).
-   Git.

### 2. Instala√ß√£o
Clone o reposit√≥rio e instale as depend√™ncias:
```bash
git clone https://github.com/lucasghidini/elt_aduaneiro
cd PROJETO ADUANEIROETL
pip install -r requirements.txt
